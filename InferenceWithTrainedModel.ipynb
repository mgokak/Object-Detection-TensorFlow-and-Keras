{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVGHywuaZi44"
      },
      "outputs": [],
      "source": [
        "# You need to update OpenCV if you are using Colab.\n",
        "# Uncomment this line if you are using Colab.\n",
        "!pip install opencv-python --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "metadata": {
        "id": "bmIc5yJvaB5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'models' in os.listdir():\n",
        "    !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "metadata": {
        "id": "mOqo3zpFaD9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The `%%bash` magic command inside a notebook lets you run a cell run like a shell interface\n",
        "# Note: the `bash` command works only on Colab.\n",
        "%%bash\n",
        "\n",
        "# Change the directory to models/research\n",
        "cd models/research/\n",
        "\n",
        "# Compile the API's Protobuf files\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Copy the required Setup file\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "\n",
        "# Install the API using the setup.py file\n",
        "python -m pip install ."
      ],
      "metadata": {
        "id": "RQH29ZvWaGnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To fix:\n",
        "# - TypeError: Descriptors cannot be created directly,\n",
        "\n",
        "!pip install protobuf==3.20.0 -q"
      ],
      "metadata": {
        "id": "6XEJmx5RaJ7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINED_MODELS = {\n",
        "                    'CenterNet_HourGlass'  : 'https://www.dropbox.com/s/oilbvq2nzkwwgg9/CenterNet_HourGlass.zip?dl=1',\n",
        "                    'EffDet_D3'            : 'https://www.dropbox.com/s/0ql06hmwnjbryzz/EffDet_D3.zip?dl=1',\n",
        "                    'FasterRCNN_ResNet101' : 'https://www.dropbox.com/s/nxv1s0geuu5pafx/FasterRCNN_ResNet101.zip?dl=1',\n",
        "                    'RetinaNet_101'        : 'https://www.dropbox.com/s/4mpu10gjosbsdbc/RetinaNet_101.zip?dl=1',\n",
        "                 }"
      ],
      "metadata": {
        "id": "tu9gH0fmaL1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Selection { display-mode: \"form\", run: \"auto\" }\n",
        "model_display_name = 'FasterRCNN_ResNet101' # @param ['CenterNet_HourGlass', 'EffDet_D3', 'FasterRCNN_ResNet101', 'RetinaNet_101']\n",
        "model_handle = TRAINED_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)"
      ],
      "metadata": {
        "id": "VOO3_PosaQiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "import requests"
      ],
      "metadata": {
        "id": "ZbtkBQctaXK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url, save_name):\n",
        "    url = url\n",
        "    file = requests.get(url)\n",
        "\n",
        "    open(save_name, 'wb').write(file.content)"
      ],
      "metadata": {
        "id": "6GsWt4TkaZVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(zip_file=None):\n",
        "    try:\n",
        "        with ZipFile(zip_file) as z:\n",
        "            z.extractall(\"./\")\n",
        "            print(\"Extracted all\")\n",
        "    except:\n",
        "        print(\"Invalid file\")"
      ],
      "metadata": {
        "id": "hXwB2znRabWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(model_display_name):\n",
        "    download_file(\n",
        "                  model_handle,\n",
        "                  f'{model_display_name}.zip'\n",
        "                 )\n",
        "    unzip(zip_file=f'{model_display_name}.zip')"
      ],
      "metadata": {
        "id": "w76fH_w2adsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# TensorFlow + Keras 2 backwards compatibility\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ],
      "metadata": {
        "id": "0HKTP8q7afop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries and files\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from object_detection.utils import (\n",
        "                                    dataset_util,\n",
        "                                    label_map_util,\n",
        "                                    config_util\n",
        "                                   )\n",
        "\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "6gT1ITvkahWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('val'):\n",
        "    download_file(\n",
        "                  'https://www.dropbox.com/s/gzb7a83ov5u1rf5/val.zip?dl=1',\n",
        "                  'val.zip'\n",
        "                 )\n",
        "    unzip(zip_file='val.zip')"
      ],
      "metadata": {
        "id": "gEQCQmAzalPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Paths of our test images.\n",
        "TEST_IMAGE_PATHS = ['val/' + f for f in  os.listdir('val') if f.endswith('.jpg')]"
      ],
      "metadata": {
        "id": "G6wtE8FWamrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This Fucntion will return a model object that can be directly\n",
        "# used to run a forward pass on new images.\n",
        "def get_model_detection_function(model):\n",
        "    \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "    @tf.function(experimental_relax_shapes=True)\n",
        "    def detect_fn(image):\n",
        "        \"\"\"\n",
        "        Returns detections and predections on an image passed to the model\n",
        "        \"\"\"\n",
        "\n",
        "        # Automatically preproccess according to the trained model configuration.\n",
        "        image, shapes = model.preprocess(image)\n",
        "\n",
        "        # Perform the prediciton.\n",
        "        prediction_dict = model.predict(image, shapes)\n",
        "\n",
        "        # Postprocess the predictions so they can be visualized.\n",
        "        detection_dict = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "        return detection_dict\n",
        "\n",
        "    return detect_fn"
      ],
      "metadata": {
        "id": "ozzgK5Raao2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_config_filename = 'pipeline.config'\n",
        "ckpt_dirname = 'training_of_model'"
      ],
      "metadata": {
        "id": "3AMIq8v8arFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt_path = os.path.join(model_display_name, ckpt_dirname)\n",
        "pipeline_config_path = os.path.join(model_display_name, pipeline_config_filename)"
      ],
      "metadata": {
        "id": "Dy01pYhrazlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the configuration file (pipeline.config) and load the model configuration.\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
        "model_config = configs['model']\n",
        "\n",
        "# Build the detection model from the models configuration.\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore the checkpoint model.\n",
        "ckpt = tf.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(tf.train.latest_checkpoint(model_ckpt_path)).expect_partial()\n",
        "\n",
        "# Call the method to create the model object.\n",
        "ckpt_inference_fn = get_model_detection_function(detection_model)"
      ],
      "metadata": {
        "id": "D7obpOJVa2L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the labelmap.\n",
        "label_map_path = os.path.join(model_display_name, configs['eval_input_config'].label_map_path)\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "\n",
        "# Create an index of the categories so our predictions can be labelled accordingly.\n",
        "category_index = label_map_util.create_category_index_from_labelmap(label_map_path, use_display_name=True)"
      ],
      "metadata": {
        "id": "MYHgUQkja7hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_visualizations_and_inf_time(image, model_det_fn, category_index, use_saved_model=True):\n",
        "\n",
        "    # Convert image into a tensor.\n",
        "    if use_saved_model:\n",
        "        input_tensor = tf.convert_to_tensor(image)\n",
        "        label_id_offset = 0\n",
        "    else:\n",
        "        input_tensor = tf.convert_to_tensor(image, dtype = tf.float32)\n",
        "        # Set a label id offset because the model will start predicitions from 0 but\n",
        "        # 0  is the background index for ckpt model format.\n",
        "        label_id_offset = 1\n",
        "\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "    start = time.time()\n",
        "    # Predict on the tensor image.\n",
        "    output_dict = model_det_fn(input_tensor)\n",
        "    end = time.time()\n",
        "\n",
        "    # Get total no. of detections in the image.\n",
        "    num_detections = int(output_dict.pop('num_detections'))\n",
        "\n",
        "    # Convert our output tensor to a numpy array.\n",
        "    output_dict = {key:value[0, :num_detections].numpy() for key,value in output_dict.items()}\n",
        "    output_dict['num_detections'] = num_detections\n",
        "\n",
        "    # Use the TensorFlow visualization_utils.py to draw the bounding boxes along with the class labels and the confidences.\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "\n",
        "      # Image to draw boxes on.\n",
        "      image,\n",
        "      # Array of Detection Boxes.\n",
        "      output_dict['detection_boxes'],\n",
        "      # Array of classes\n",
        "      (output_dict['detection_classes'] + label_id_offset).astype(int),\n",
        "      # Array of prediction scores of each detection.\n",
        "      output_dict['detection_scores'],\n",
        "      # Category index from label map.\n",
        "      category_index,\n",
        "      # Normalize the coodindates.\n",
        "      use_normalized_coordinates=True,\n",
        "      # Max boxes to draw on the image.\n",
        "      max_boxes_to_draw=30,\n",
        "      # Minimum level of confidence for each detection to be considered valid.\n",
        "      min_score_thresh=.50,\n",
        "      # Show classes with scores.\n",
        "      agnostic_mode=False)\n",
        "\n",
        "    return image, end-start"
      ],
      "metadata": {
        "id": "V5aB5J7ha9Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, image_path in enumerate(TEST_IMAGE_PATHS):\n",
        "\n",
        "    # Read the image as a numpy array.\n",
        "    image = np.array(Image.open(image_path))\n",
        "\n",
        "    image_pred, _ = get_visualizations_and_inf_time(image, ckpt_inference_fn, category_index, use_saved_model=False)\n",
        "\n",
        "    # Displaying the image along with bounding box predictions.\n",
        "    display(Image.fromarray(image_pred))\n",
        "\n",
        "    if i>10:\n",
        "        break"
      ],
      "metadata": {
        "id": "VrWag7KUbDTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory of your Trained model\n",
        "output_directory = 'saved_model'"
      ],
      "metadata": {
        "id": "Og7GU2RYbDzO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the script to export the model into saved_model format\n",
        "# You will need to pass, your trained model ckpt directory, pipeline path, output dir.\n",
        "!python ./models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {model_ckpt_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_config_path}"
      ],
      "metadata": {
        "id": "6RV3OuLCbOFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to load model from the Saved Model\n",
        "def load_saved_model(trained_model_path):\n",
        "    '''\n",
        "    Args:\n",
        "    trained_model_path: Directory where saved_model is saved.\n",
        "    '''\n",
        "\n",
        "    # Load model from the training directory\n",
        "    model_dir = os.path.join(trained_model_path, \"saved_model\")\n",
        "    model = tf.saved_model.load(model_dir)\n",
        "    model = model.signatures['serving_default']\n",
        "    return model"
      ],
      "metadata": {
        "id": "ePFDLu5vbS6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the load model method to load the trained model\n",
        "saved_model_inference_fn = load_saved_model(output_directory)"
      ],
      "metadata": {
        "id": "O8Qp5GvcbVSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all test images and predict.\n",
        "for i, image_path in enumerate(TEST_IMAGE_PATHS):\n",
        "\n",
        "    # Read the image as a numpy array.\n",
        "    image = np.array(Image.open(image_path))\n",
        "\n",
        "    image_pred, _ = get_visualizations_and_inf_time(image, saved_model_inference_fn, category_index, use_saved_model=True)\n",
        "\n",
        "    # Displaying the image along with bounding box predictions.\n",
        "    display(Image.fromarray(image_pred))\n",
        "\n",
        "    if i>10:\n",
        "        break"
      ],
      "metadata": {
        "id": "QjS5Y2KtbXY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_on_video(video_path, model_det_fn, model_name, category_index):\n",
        "    \"\"\"\n",
        "    Read video frames one-by-one, flip it, and write in the other video.\n",
        "    \"\"\"\n",
        "\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if camera opened successfully\n",
        "    if not video.isOpened():\n",
        "        print(\"Error opening video file\")\n",
        "        return\n",
        "\n",
        "    # create video writer\n",
        "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    output_fname = '{}_{}_out.mp4'.format(os.path.splitext(video_path)[0], model_name)\n",
        "\n",
        "    output_file = cv2.VideoWriter(\n",
        "        filename=output_fname,\n",
        "        fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "        fps=float(frames_per_second),\n",
        "        frameSize=(width, height),\n",
        "        isColor=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    while video.isOpened():\n",
        "        ret, frame = video.read()\n",
        "        if ret:\n",
        "            output_frame, inf_time = get_visualizations_and_inf_time(frame[...,::-1], model_det_fn, category_index, use_saved_model=True)\n",
        "            fps = 1./ inf_time\n",
        "            disp_fps = \"FPS: \"+ str(round(fps,2))\n",
        "            output_frame = np.ascontiguousarray(output_frame)\n",
        "            output_frame = cv2.putText(output_frame, disp_fps, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (100, 255, 255), 3)\n",
        "            output_frame = cv2.putText(output_frame, f'Model: {model_name}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (100, 255, 255), 3)\n",
        "\n",
        "            output_file.write(output_frame[...,::-1])\n",
        "\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    video.release()\n",
        "    output_file.release()\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "GJz7URjAbiEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_file = \"video_sample.mp4\"\n",
        "inference_on_video(video_file, saved_model_inference_fn, model_display_name, category_index)"
      ],
      "metadata": {
        "id": "y0b7TEHrblZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"X2wnWkT5XsE\", width=800, height=450)\n",
        "display(video)"
      ],
      "metadata": {
        "id": "k9BeXJrXbodF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}